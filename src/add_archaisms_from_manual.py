#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∞—Ä—Ö–∞–∏–∑–º–æ–≤ –∏–∑ —Ä—É—á–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ (—É—Å—Ç–∞—Ä–µ–≤—à–∏–µ_–∏–∑_–æ–±–ª–∞–∫–∞.txt)
–≤ –æ—Å–Ω–æ–≤–Ω–æ–π —Å–ª–æ–≤–∞—Ä—å –∞—Ä—Ö–∞–∏–∑–º–æ–≤.
"""

import pandas as pd
from pathlib import Path


def load_manual_archaisms(file_path):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∞—Ä—Ö–∞–∏–∑–º–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞.
    –£–±–∏—Ä–∞–µ—Ç –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –ø—Ä–æ–±–µ–ª—ã.
    """
    print(f"–ó–∞–≥—Ä—É–∂–∞—é —Ä—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫ –∏–∑ {file_path}...")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    # –û—á–∏—â–∞–µ–º –æ—Ç –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫
    words = []
    for line in lines:
        word = line.strip()
        if word:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
            words.append(word.lower())
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    unique_words = sorted(set(words))
    
    print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(words)} —Å–ª–æ–≤ ({len(unique_words)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö)")
    return unique_words


def load_existing_archaisms(csv_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Å–ª–æ–≤–∞—Ä—å –∞—Ä—Ö–∞–∏–∑–º–æ–≤."""
    print(f"\n–ó–∞–≥—Ä—É–∂–∞—é —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Å–ª–æ–≤–∞—Ä—å –∏–∑ {csv_path}...")
    df = pd.read_csv(csv_path)
    print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
    
    # –ü–æ–ª—É—á–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤—Å–µ—Ö —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–ª–æ–≤ (–æ—Å–Ω–æ–≤–Ω—ã—Ö + –≤–∞—Ä–∏–∞–Ω—Ç—ã)
    existing_words = set()
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ —Å–ª–æ–≤–∞
    existing_words.update(df['word'].str.lower())
    
    # –í–∞—Ä–∏–∞–Ω—Ç—ã (–µ—Å–ª–∏ –µ—Å—Ç—å)
    for variants in df['variants'].dropna():
        for variant in variants.split(','):
            word = variant.strip().lower()
            if word:
                existing_words.add(word)
    
    print(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ (—Å –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏): {len(existing_words)}")
    return df, existing_words


def add_new_archaisms(df_existing, existing_words, new_words):
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ –∞—Ä—Ö–∞–∏–∑–º—ã –≤ DataFrame.
    
    Returns:
        tuple: (–æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π DataFrame, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤)
    """
    print("\n" + "="*60)
    print("–ê–ù–ê–õ–ò–ó –ù–û–í–´–• –°–õ–û–í")
    print("="*60)
    
    words_to_add = []
    already_exist = []
    
    for word in new_words:
        if word in existing_words:
            already_exist.append(word)
        else:
            words_to_add.append(word)
    
    print(f"\n–í—Å–µ–≥–æ —Å–ª–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏: {len(new_words)}")
    print(f"  ‚úì –£–∂–µ –µ—Å—Ç—å –≤ —Å–ª–æ–≤–∞—Ä–µ: {len(already_exist)}")
    print(f"  + –ù–æ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è: {len(words_to_add)}")
    
    if already_exist:
        print(f"\n  –°–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ –µ—Å—Ç—å:")
        for word in already_exist[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
            print(f"    - {word}")
        if len(already_exist) > 10:
            print(f"    ... –∏ –µ—â—ë {len(already_exist) - 10} —Å–ª–æ–≤")
    
    if words_to_add:
        print(f"\n  –ù–æ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è:")
        for word in words_to_add:
            print(f"    + {word}")
    
    # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
    new_rows = []
    for word in words_to_add:
        new_rows.append({
            'word': word.capitalize(),  # –ü—Ä–∏–≤–æ–¥–∏–º –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –≤–∏–¥—É
            'definition': '–î–æ–±–∞–≤–ª–µ–Ω–æ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞ –æ–±–ª–∞–∫–æ–≤ —Å–ª–æ–≤ (—Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)',
            'variants': None,
            'letter': word[0].upper(),
            'original': word.capitalize()
        })
    
    if new_rows:
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ –≤ DataFrame
        df_new = pd.DataFrame(new_rows)
        df_updated = pd.concat([df_existing, df_new], ignore_index=True)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å–ª–æ–≤—É
        df_updated = df_updated.sort_values('word').reset_index(drop=True)
    else:
        df_updated = df_existing
    
    return df_updated, len(words_to_add)


def update_wordlist(wordlist_path, df):
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º —Å–ª–æ–≤.
    """
    print(f"\n–û–±–Ω–æ–≤–ª—è—é —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤: {wordlist_path}...")
    
    all_words = set()
    
    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Å–ª–æ–≤–∞
    all_words.update(df['word'].str.lower())
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã
    for variants in df['variants'].dropna():
        for variant in variants.split(','):
            word = variant.strip().lower()
            if word:
                all_words.add(word)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
    with open(wordlist_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(sorted(all_words)))
    
    print(f"‚úì –û–±–Ω–æ–≤–ª–µ–Ω–æ: {len(all_words)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤")


def main():
    # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
    project_root = Path(__file__).parent.parent
    manual_file = project_root / 'dataset' / '—É—Å—Ç–∞—Ä–µ–≤—à–∏–µ_–∏–∑_–æ–±–ª–∞–∫–∞.txt'
    csv_path = project_root / 'dataset' / 'archaisms.csv'
    parquet_path = project_root / 'dataset' / 'archaisms.parquet'
    wordlist_path = project_root / 'dataset' / 'archaisms_wordlist.txt'
    
    print("="*60)
    print("–î–û–ë–ê–í–õ–ï–ù–ò–ï –ê–†–•–ê–ò–ó–ú–û–í –ò–ó –†–£–ß–ù–û–ì–û –°–ü–ò–°–ö–ê")
    print("="*60 + "\n")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞
    if not manual_file.exists():
        print(f"–û–®–ò–ë–ö–ê: –§–∞–π–ª {manual_file} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return 1
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    new_words = load_manual_archaisms(manual_file)
    df_existing, existing_words = load_existing_archaisms(csv_path)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Å–ª–æ–≤–∞
    df_updated, added_count = add_new_archaisms(df_existing, existing_words, new_words)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    if added_count > 0:
        print("\n" + "="*60)
        print("–°–û–•–†–ê–ù–ï–ù–ò–ï –û–ë–ù–û–í–õ–Å–ù–ù–û–ì–û –°–õ–û–í–ê–†–Ø")
        print("="*60)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV
        print(f"\n–°–æ—Ö—Ä–∞–Ω—è—é CSV: {csv_path}")
        df_updated.to_csv(csv_path, index=False, encoding='utf-8')
        csv_size_kb = csv_path.stat().st_size / 1024
        print(f"  ‚úì CSV —Å–æ—Ö—Ä–∞–Ω—ë–Ω ({csv_size_kb:.2f} KB)")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º Parquet
        print(f"\n–°–æ—Ö—Ä–∞–Ω—è—é Parquet: {parquet_path}")
        df_updated.to_parquet(parquet_path, index=False)
        parquet_size_kb = parquet_path.stat().st_size / 1024
        print(f"  ‚úì Parquet —Å–æ—Ö—Ä–∞–Ω—ë–Ω ({parquet_size_kb:.2f} KB)")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤
        update_wordlist(wordlist_path, df_updated)
    else:
        print("\n" + "="*60)
        print("‚ö† –ù–æ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–µ—Ç")
        print("="*60)
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "="*60)
    print("–ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("="*60)
    print(f"\n–ë—ã–ª–æ –∑–∞–ø–∏—Å–µ–π –≤ —Å–ª–æ–≤–∞—Ä–µ: {len(df_existing)}")
    print(f"–°—Ç–∞–ª–æ –∑–∞–ø–∏—Å–µ–π –≤ —Å–ª–æ–≤–∞—Ä–µ: {len(df_updated)}")
    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ {len(new_words)} —Å–ª–æ–≤:")
    print(f"  ‚úì –£–∂–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞–ª–∏: {len(new_words) - added_count}")
    print(f"  + –î–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö: {added_count}")
    
    if added_count > 0:
        print(f"\n‚úì –°–ª–æ–≤–∞—Ä—å —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª—ë–Ω!")
    else:
        print(f"\n‚úì –°–ª–æ–≤–∞—Ä—å –Ω–µ –∏–∑–º–µ–Ω—ë–Ω (–≤—Å–µ —Å–ª–æ–≤–∞ —É–∂–µ –±—ã–ª–∏)")
    print("="*60 + "\n")


if __name__ == '__main__':
    main()

